{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why I like Julia"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Coding experience!\n",
    "\n",
    "- Amazing IDE with VS code and inline prompts\n",
    "\n",
    "![](https://code.visualstudio.com/assets/docs/languages/julia/overview.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Package management\n",
    "\n",
    "Julia provides a **built-in package** manager called `Pkg` for managing packages and environments. Users can create a new environment and add specific packages to it, and each environment has its own set of dependencies. Julia also allows users to switch between different environments easily.\n",
    "\n",
    "Environments is tracked through a simple `Manifest.toml` and `Project.toml`, making Julia scripts fully reproducible.\n",
    "\n",
    "In comparision, \n",
    "\n",
    "More on that later on!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Execution speed!\n",
    "\n",
    "Let's construct a for loop summation of a random sequence of integers from 1 to 1,000,000,000 (1 billion) that are sampled without replacment.1 Here is the correct answer as a reference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using StatsBase, BenchmarkTools\n",
    "n = 1_000_000_000;\n",
    "function sum_n()\n",
    "    s = 0\n",
    "    for i in 1:n\n",
    "        s = s + i\n",
    "    end\n",
    "    return s\n",
    "end\n",
    "@btime sum_n(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using RCall\n",
    "R\"\"\"\n",
    "n = 1000000000\n",
    "sum_n = function(){\n",
    "    s = 0\n",
    "    for (i in n){\n",
    "        s = s + i\n",
    "    }\n",
    "    s\n",
    "}\n",
    "print(system.time(x <- sum_n()))\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Community well organized\n",
    "\n",
    "- Slack channel\n",
    "- Discourse forum\n",
    "- Youtube tutorials\n",
    "\n",
    "Information available at https://julialang.org/community/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Multiple Dispatch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract type PlantSpecies end\n",
    "\n",
    "struct Oak <: PlantSpecies\n",
    "    height::Float64\n",
    "    leaf_area::Float64\n",
    "end\n",
    "\n",
    "struct Maple <: PlantSpecies\n",
    "    height::Float64\n",
    "    leaf_area::Float64\n",
    "end\n",
    "\n",
    "function aboveground_biomass(species::Oak)\n",
    "    return 0.0314 * species.height^2.19 * species.leaf_area^0.91\n",
    "end\n",
    "\n",
    "function aboveground_biomass(species::Maple)\n",
    "    return 0.0215 * species.height^2.42 * species.leaf_area^0.94\n",
    "end\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More interesting use case of multiple dispatch: GPU computing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple dispatch can be useful when it comes to GPU computing because it allows for efficient dispatch of functions to the GPU, which can lead to significant performance gains.\n",
    "\n",
    "In GPU computing, data parallelism is often used to perform computations on large arrays in parallel. This involves splitting the input data into smaller chunks and executing the same code on each chunk concurrently. To take advantage of GPU hardware, computations need to be executed on the GPU in parallel.\n",
    "\n",
    "Julia's multiple dispatch mechanism allows functions to be specialized for different types of data and to dispatch the computation to the GPU when appropriate. This is achieved through Julia's `GPUArrays` package, which provides a GPU-backed array type that can be used in place of regular Julia arrays.\n",
    "\n",
    "When a function is called on a `GPUArray`, Julia's multiple dispatch mechanism can determine if a GPU version of the function is available, and if so, dispatch the computation to the GPU. This allows computations to be executed in parallel on the GPU, which can lead to significant performance gains over executing the same computation on the CPU.\n",
    "\n",
    "Here's an example:\n",
    "\n",
    "```julia\n",
    "using CUDA\n",
    "\n",
    "function add_matrices(a::AbstractArray, b::AbstractArray)\n",
    "    return a + b\n",
    "end\n",
    "\n",
    "# generate CPU arrays\n",
    "a = rand(1000, 1000)\n",
    "b = rand(1000, 1000)\n",
    "\n",
    "# call the function on CPU arrays\n",
    "c = add_matrices(a, b)\n",
    "\n",
    "# generate GPU arrays\n",
    "a_gpu = CUDA.rand(1000, 1000)\n",
    "b_gpu = CUDA.rand(1000, 1000)\n",
    "\n",
    "# call the function on GPU arrays\n",
    "c_gpu = add_matrices(a_gpu, b_gpu)\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "In this example, we define a function `add_matrices` that adds two arrays together. When called with `a_gpu` and `b_gpu`, Julia's multiple dispatch mechanism recognizes that a GPU version of the function is available and dispatches the computation to the GPU.\n",
    "\n",
    "The resulting `c_gpu` array contains the result of the computation, which can be copied back to the CPU using the `Array` function.\n",
    "\n",
    "In summary, multiple dispatch in Julia can be useful when it comes to GPU computing because it allows functions to be specialized for different types of data and to dispatch computations to the GPU when appropriate. This can lead to significant performance gains, especially when working with large arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Productivity\n",
    "\n",
    "- Code can be made very generic\n",
    "  - For instance, code can be very effortlessly used for GPU computing\n",
    "- Research script can be easily transformed into **packages**, directly available to the community"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Understandability\n",
    "\n",
    "- Source code of most Julia libraries is written in pure Julia"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Composability of libraries and scientific ML\n",
    "-  automatic differentiation pervasive language (Innes et al., 2019)\n",
    "### Deep Learning libraries in combination with ODE solvers\n",
    "\n",
    "Inspired from https://github.com/ChrisRackauckas/universal_differential_equations/blob/d622d92095775f1f2bb3eee8f6c0e443a7f1ae34/LotkaVolterra/scenario_1.jl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd(@__DIR__)\n",
    "using Pkg; Pkg.activate(\".\"); Pkg.instantiate()\n",
    "\n",
    "using OrdinaryDiffEq\n",
    "using ModelingToolkit\n",
    "using DataDrivenDiffEq\n",
    "using LinearAlgebra, ComponentArrays\n",
    "using Optimization, OptimizationOptimisers, OptimizationOptimJL #OptimizationFlux for ADAM and OptimizationOptimJL for BFGS\n",
    "using DiffEqSensitivity\n",
    "using Lux\n",
    "using Plots\n",
    "gr()\n",
    "using JLD2, FileIO\n",
    "using Statistics\n",
    "# Set a random seed for reproduceable behaviour\n",
    "using Random\n",
    "rng = Random.default_rng()\n",
    "Random.seed!(1234)\n",
    "\n",
    "#### NOTE\n",
    "# Since the recent release of DataDrivenDiffEq v0.6.0 where a complete overhaul of the optimizers took\n",
    "# place, SR3 has been used. Right now, STLSQ performs better and has been changed.\n",
    "\n",
    "# Create a name for saving ( basically a prefix )\n",
    "svname = \"Scenario_1_\"\n",
    "\n",
    "## Data generation\n",
    "function lotka!(du, u, p, t)\n",
    "    α, β, γ, δ = p\n",
    "    du[1] = α*u[1] - β*u[2]*u[1]\n",
    "    du[2] = γ*u[1]*u[2]  - δ*u[2]\n",
    "end\n",
    "\n",
    "# Define the experimental parameter\n",
    "tspan = (0.0,3.0)\n",
    "u0 = [0.44249296,4.6280594]\n",
    "p_ = [1.3, 0.9, 0.8, 1.8]\n",
    "prob = ODEProblem(lotka!, u0,tspan, p_)\n",
    "solution = solve(prob, Vern7(), abstol=1e-12, reltol=1e-12, saveat = 0.1)\n",
    "\n",
    "# Ideal data\n",
    "X = Array(solution)\n",
    "t = solution.t\n",
    "DX = Array(solution(solution.t, Val{1}))\n",
    "\n",
    "full_problem = DataDrivenProblem(X, t = t, DX = DX)\n",
    "\n",
    "# Add noise in terms of the mean\n",
    "x̄ = mean(X, dims = 2)\n",
    "noise_magnitude = 5e-3\n",
    "Xₙ = X .+ (noise_magnitude*x̄) .* randn(eltype(X), size(X))\n",
    "\n",
    "plot(solution, alpha = 0.75, color = :black, label = [\"True Data\" nothing])\n",
    "scatter!(t, transpose(Xₙ), color = :red, label = [\"Noisy Data\" nothing])\n",
    "## Define the network\n",
    "# Gaussian RBF as activation\n",
    "rbf(x) = exp.(-(x.^2))\n",
    "\n",
    "# Multilayer FeedForward\n",
    "U = Lux.Chain(\n",
    "    Lux.Dense(2,5,rbf), Lux.Dense(5,5, rbf), Lux.Dense(5,5, rbf), Lux.Dense(5,2)\n",
    ")\n",
    "# Get the initial parameters and state variables of the model\n",
    "p, st = Lux.setup(rng, U)\n",
    "\n",
    "# Define the hybrid model\n",
    "function ude_dynamics!(du,u, p, t, p_true)\n",
    "    û = U(u, p, st)[1] # Network prediction\n",
    "    du[1] = p_true[1]*u[1] + û[1]\n",
    "    du[2] = -p_true[4]*u[2] + û[2]\n",
    "end\n",
    "\n",
    "# Closure with the known parameter\n",
    "nn_dynamics!(du,u,p,t) = ude_dynamics!(du,u,p,t,p_)\n",
    "# Define the problem\n",
    "prob_nn = ODEProblem(nn_dynamics!,Xₙ[:, 1], tspan, p)\n",
    "\n",
    "## Function to train the network\n",
    "# Define a predictor\n",
    "function predict(θ, X = Xₙ[:,1], T = t)\n",
    "    _prob = remake(prob_nn, u0 = X, tspan = (T[1], T[end]), p = θ)\n",
    "    Array(solve(_prob, Vern7(), saveat = T,\n",
    "                abstol=1e-6, reltol=1e-6,\n",
    "                sensealg = ForwardDiffSensitivity()\n",
    "                ))\n",
    "end\n",
    "\n",
    "# Simple L2 loss\n",
    "function loss(θ)\n",
    "    X̂ = predict(θ)\n",
    "    sum(abs2, Xₙ .- X̂)\n",
    "end\n",
    "\n",
    "# Container to track the losses\n",
    "losses = Float64[]\n",
    "\n",
    "callback = function (p, l)\n",
    "  push!(losses, l)\n",
    "  if length(losses)%50==0\n",
    "      println(\"Current loss after $(length(losses)) iterations: $(losses[end])\")\n",
    "  end\n",
    "  return false\n",
    "end\n",
    "\n",
    "## Training\n",
    "\n",
    "# First train with ADAM for better convergence -> move the parameters into a\n",
    "# favourable starting positing for BFGS\n",
    "adtype = Optimization.AutoZygote()\n",
    "optf = Optimization.OptimizationFunction((x,p)->loss(x), adtype)\n",
    "optprob = Optimization.OptimizationProblem(optf, ComponentVector{Float64}(p))\n",
    "res1 = Optimization.solve(optprob, ADAM(0.1), callback=callback, maxiters = 200)\n",
    "println(\"Training loss after $(length(losses)) iterations: $(losses[end])\")\n",
    "# Train with BFGS\n",
    "optprob2 = Optimization.OptimizationProblem(optf, res1.minimizer)\n",
    "res2 = Optimization.solve(optprob2, Optim.BFGS(initial_stepnorm=0.01), callback=callback, maxiters = 10000)\n",
    "println(\"Final training loss after $(length(losses)) iterations: $(losses[end])\")\n",
    "\n",
    "# Plot the losses\n",
    "pl_losses = plot(1:200, losses[1:200], yaxis = :log10, xaxis = :log10, xlabel = \"Iterations\", ylabel = \"Loss\", label = \"ADAM\", color = :blue)\n",
    "plot!(201:length(losses), losses[201:end], yaxis = :log10, xaxis = :log10, xlabel = \"Iterations\", ylabel = \"Loss\", label = \"BFGS\", color = :red)\n",
    "savefig(pl_losses, joinpath(pwd(), \"plots\", \"$(svname)_losses.pdf\"))\n",
    "# Rename the best candidate\n",
    "p_trained = res2.minimizer\n",
    "\n",
    "## Analysis of the trained network\n",
    "# Plot the data and the approximation\n",
    "ts = first(solution.t):mean(diff(solution.t))/2:last(solution.t)\n",
    "X̂ = predict(p_trained, Xₙ[:,1], ts)\n",
    "# Trained on noisy data vs real solution\n",
    "pl_trajectory = plot(ts, transpose(X̂), xlabel = \"t\", ylabel =\"x(t), y(t)\", color = :red, label = [\"UDE Approximation\" nothing])\n",
    "scatter!(solution.t, transpose(Xₙ), color = :black, label = [\"Measurements\" nothing])\n",
    "savefig(pl_trajectory, joinpath(pwd(), \"plots\", \"$(svname)_trajectory_reconstruction.pdf\"))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian inference framework with deep learning\n",
    "\n",
    "inspired from https://turinglang.org/dev/tutorials/03-bayesian-neural-network/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Turing\n",
    "using FillArrays\n",
    "using Flux\n",
    "using Plots\n",
    "using ReverseDiff\n",
    "\n",
    "using LinearAlgebra\n",
    "using Random\n",
    "\n",
    "# Use reverse_diff due to the number of parameters in neural networks.\n",
    "Turing.setadbackend(:reversediff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of points to generate.\n",
    "N = 80\n",
    "M = round(Int, N / 4)\n",
    "Random.seed!(1234)\n",
    "\n",
    "# Generate artificial data.\n",
    "x1s = rand(M) * 4.5;\n",
    "x2s = rand(M) * 4.5;\n",
    "xt1s = Array([[x1s[i] + 0.5; x2s[i] + 0.5] for i in 1:M])\n",
    "x1s = rand(M) * 4.5;\n",
    "x2s = rand(M) * 4.5;\n",
    "append!(xt1s, Array([[x1s[i] - 5; x2s[i] - 5] for i in 1:M]))\n",
    "\n",
    "x1s = rand(M) * 4.5;\n",
    "x2s = rand(M) * 4.5;\n",
    "xt0s = Array([[x1s[i] + 0.5; x2s[i] - 5] for i in 1:M])\n",
    "x1s = rand(M) * 4.5;\n",
    "x2s = rand(M) * 4.5;\n",
    "append!(xt0s, Array([[x1s[i] - 5; x2s[i] + 0.5] for i in 1:M]))\n",
    "\n",
    "# Store all the data for later.\n",
    "xs = [xt1s; xt0s]\n",
    "ts = [ones(2 * M); zeros(2 * M)]\n",
    "\n",
    "# Plot data points.\n",
    "function plot_data()\n",
    "    x1 = map(e -> e[1], xt1s)\n",
    "    y1 = map(e -> e[2], xt1s)\n",
    "    x2 = map(e -> e[1], xt0s)\n",
    "    y2 = map(e -> e[2], xt0s)\n",
    "\n",
    "    Plots.scatter(x1, y1; color=\"red\", clim=(0, 1))\n",
    "    return Plots.scatter!(x2, y2; color=\"blue\", clim=(0, 1))\n",
    "end\n",
    "\n",
    "plot_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a neural network using Flux\n",
    "nn_initial = Chain(Dense(2, 3, tanh), Dense(3, 2, tanh), Dense(2, 1, σ))\n",
    "\n",
    "# Extract weights and a helper function to reconstruct NN from weights\n",
    "parameters_initial, reconstruct = Flux.destructure(nn_initial)\n",
    "\n",
    "length(parameters_initial) # number of paraemters in NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@model function bayes_nn(xs, ts, nparameters, reconstruct; alpha=0.09)\n",
    "    # Create the weight and bias vector.\n",
    "    parameters ~ MvNormal(Zeros(nparameters), I / alpha)\n",
    "\n",
    "    # Construct NN from parameters\n",
    "    nn = reconstruct(parameters)\n",
    "    # Forward NN to make predictions\n",
    "    preds = nn(xs)\n",
    "\n",
    "    # Observe each prediction.\n",
    "    for i in 1:length(ts)\n",
    "        ts[i] ~ Bernoulli(preds[i])\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform inference.\n",
    "N = 5000\n",
    "ch = sample(bayes_nn(hcat(xs...), ts, length(parameters_initial), reconstruct), NUTS(), N);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper to create NN from weights `theta` and run it through data `x`\n",
    "nn_forward(x, theta) = reconstruct(theta)(x)\n",
    "\n",
    "# Plot the data we have.\n",
    "plot_data()\n",
    "\n",
    "# Find the index that provided the highest log posterior in the chain.\n",
    "_, i = findmax(ch[:lp])\n",
    "\n",
    "# Extract the max row value from i.\n",
    "i = i.I[1]\n",
    "\n",
    "# Plot the posterior distribution with a contour plot\n",
    "x1_range = collect(range(-6; stop=6, length=25))\n",
    "x2_range = collect(range(-6; stop=6, length=25))\n",
    "Z = [nn_forward([x1, x2], theta[i, :])[1] for x1 in x1_range, x2 in x2_range]\n",
    "contour!(x1_range, x2_range, Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return the average predicted value across\n",
    "# multiple weights.\n",
    "function nn_predict(x, theta, num)\n",
    "    return mean([nn_forward(x, theta[i, :])[1] for i in 1:10:num])\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the average prediction.\n",
    "plot_data()\n",
    "\n",
    "n_end = 1500\n",
    "x1_range = collect(range(-6; stop=6, length=25))\n",
    "x2_range = collect(range(-6; stop=6, length=25))\n",
    "Z = [nn_predict([x1, x2], theta, n_end)[1] for x1 in x1_range, x2 in x2_range]\n",
    "contour!(x1_range, x2_range, Z)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A few packages that I have developed\n",
    "\n",
    "### [EvoId.jl](https://github.com/vboussange/EvoId.jl)\n",
    "Evolutionary Individual based modelling, mathematically grounded. A user friendly package aimed at simulating the evolutionary dynamics of a population structured over a complex spatio-evolutionary structures.\n",
    "\n",
    "### [HighDimPDE.jl](https://github.com/SciML/HighDimPDE.jl)\n",
    "Solver for **highly dimensional, non-local, nonlinear PDEs**. It is integrated within the SciML ecosystem (see below). Try it out! &#128515; If you want to learn more about the algorithms implemented, check out my [research interests]({{site.url}}/research/#developping-numerical-schemes-for-solving-high-dimensional-non-local-nonlinear-pdes).\n",
    "\n",
    "### [PiecewiseInference.jl](https://github.com/vboussange/PiecewiseInference.jl)\n",
    "Suite for parameter inference and model selection with dynamical models characterised by complex dynamics.\n",
    "\n",
    "### [ParametricModels.jl](https://github.com/vboussange/ParametricModels.jl)\n",
    "Utilities for parametric and composite differential equation models.\n",
    "\n",
    "### [EcoEvoModelZoo.jl](https://github.com/vboussange/EcoEvoModelZoo.jl)\n",
    "A zoo of eco-evolutionary models with high fitness.\n",
    "\n",
    "### [SciML](https://github.com/SciML/)\n",
    "I am a member of the **SciML** organisation, an open source ecosystem for Scientific Machine Learning in the Julia programming language. On top of being the main author of **HighDimPDE.jl**, I actively participate in the development of other packages such as [DiffEqFlux.jl](https://github.com/SciML/DiffEqFlux.jl), a library to train differential equations with data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.5",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
