{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why I like Julia"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Coding experience!\n",
    "\n",
    "- Amazing IDE with VS code and inline prompts\n",
    "\n",
    "![](https://user-images.githubusercontent.com/1814174/88589293-e8207f80-d026-11ea-86e2-8a3feb8252ca.gif)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Package management\n",
    "\n",
    "Environment is tracked through a simple `Manifest.toml` and `Project.toml`, making Julia scripts fully reproducible."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Community well organized\n",
    "\n",
    "- Slack channel\n",
    "- Discourse forum\n",
    "- Youtube tutorials\n",
    "\n",
    "Information available at https://julialang.org/community/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Productivity\n",
    "\n",
    "- Code can be made very generic\n",
    "  - For instance, code can be very effortlessly used for GPU computing\n",
    "- Research script can be easily transformed into **packages**, directly available to the community"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Understandability\n",
    "\n",
    "- Source code of most Julia libraries is written in pure Julia"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Composability of libraries and scientific ML\n",
    "-  automatic differentiation pervasive language (Innes et al., 2019)\n",
    "### Deep Learning libraries in combination with ODE solvers\n",
    "\n",
    "Inspired from https://github.com/ChrisRackauckas/universal_differential_equations/blob/d622d92095775f1f2bb3eee8f6c0e443a7f1ae34/LotkaVolterra/scenario_1.jl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [],
   "source": [
    "cd(@__DIR__)\n",
    "using Pkg; Pkg.activate(\".\"); Pkg.instantiate()\n",
    "\n",
    "using OrdinaryDiffEq\n",
    "using ModelingToolkit\n",
    "using DataDrivenDiffEq\n",
    "using LinearAlgebra, ComponentArrays\n",
    "using Optimization, OptimizationOptimisers, OptimizationOptimJL #OptimizationFlux for ADAM and OptimizationOptimJL for BFGS\n",
    "using DiffEqSensitivity\n",
    "using Lux\n",
    "using Plots\n",
    "gr()\n",
    "using JLD2, FileIO\n",
    "using Statistics\n",
    "# Set a random seed for reproduceable behaviour\n",
    "using Random\n",
    "rng = Random.default_rng()\n",
    "Random.seed!(1234)\n",
    "\n",
    "#### NOTE\n",
    "# Since the recent release of DataDrivenDiffEq v0.6.0 where a complete overhaul of the optimizers took\n",
    "# place, SR3 has been used. Right now, STLSQ performs better and has been changed.\n",
    "\n",
    "# Create a name for saving ( basically a prefix )\n",
    "svname = \"Scenario_1_\"\n",
    "\n",
    "## Data generation\n",
    "function lotka!(du, u, p, t)\n",
    "    α, β, γ, δ = p\n",
    "    du[1] = α*u[1] - β*u[2]*u[1]\n",
    "    du[2] = γ*u[1]*u[2]  - δ*u[2]\n",
    "end\n",
    "\n",
    "# Define the experimental parameter\n",
    "tspan = (0.0,3.0)\n",
    "u0 = [0.44249296,4.6280594]\n",
    "p_ = [1.3, 0.9, 0.8, 1.8]\n",
    "prob = ODEProblem(lotka!, u0,tspan, p_)\n",
    "solution = solve(prob, Vern7(), abstol=1e-12, reltol=1e-12, saveat = 0.1)\n",
    "\n",
    "# Ideal data\n",
    "X = Array(solution)\n",
    "t = solution.t\n",
    "DX = Array(solution(solution.t, Val{1}))\n",
    "\n",
    "full_problem = DataDrivenProblem(X, t = t, DX = DX)\n",
    "\n",
    "# Add noise in terms of the mean\n",
    "x̄ = mean(X, dims = 2)\n",
    "noise_magnitude = 5e-3\n",
    "Xₙ = X .+ (noise_magnitude*x̄) .* randn(eltype(X), size(X))\n",
    "\n",
    "plot(solution, alpha = 0.75, color = :black, label = [\"True Data\" nothing])\n",
    "scatter!(t, transpose(Xₙ), color = :red, label = [\"Noisy Data\" nothing])\n",
    "## Define the network\n",
    "# Gaussian RBF as activation\n",
    "rbf(x) = exp.(-(x.^2))\n",
    "\n",
    "# Multilayer FeedForward\n",
    "U = Lux.Chain(\n",
    "    Lux.Dense(2,5,rbf), Lux.Dense(5,5, rbf), Lux.Dense(5,5, rbf), Lux.Dense(5,2)\n",
    ")\n",
    "# Get the initial parameters and state variables of the model\n",
    "p, st = Lux.setup(rng, U)\n",
    "\n",
    "# Define the hybrid model\n",
    "function ude_dynamics!(du,u, p, t, p_true)\n",
    "    û = U(u, p, st)[1] # Network prediction\n",
    "    du[1] = p_true[1]*u[1] + û[1]\n",
    "    du[2] = -p_true[4]*u[2] + û[2]\n",
    "end\n",
    "\n",
    "# Closure with the known parameter\n",
    "nn_dynamics!(du,u,p,t) = ude_dynamics!(du,u,p,t,p_)\n",
    "# Define the problem\n",
    "prob_nn = ODEProblem(nn_dynamics!,Xₙ[:, 1], tspan, p)\n",
    "\n",
    "## Function to train the network\n",
    "# Define a predictor\n",
    "function predict(θ, X = Xₙ[:,1], T = t)\n",
    "    _prob = remake(prob_nn, u0 = X, tspan = (T[1], T[end]), p = θ)\n",
    "    Array(solve(_prob, Vern7(), saveat = T,\n",
    "                abstol=1e-6, reltol=1e-6,\n",
    "                sensealg = ForwardDiffSensitivity()\n",
    "                ))\n",
    "end\n",
    "\n",
    "# Simple L2 loss\n",
    "function loss(θ)\n",
    "    X̂ = predict(θ)\n",
    "    sum(abs2, Xₙ .- X̂)\n",
    "end\n",
    "\n",
    "# Container to track the losses\n",
    "losses = Float64[]\n",
    "\n",
    "callback = function (p, l)\n",
    "  push!(losses, l)\n",
    "  if length(losses)%50==0\n",
    "      println(\"Current loss after $(length(losses)) iterations: $(losses[end])\")\n",
    "  end\n",
    "  return false\n",
    "end\n",
    "\n",
    "## Training\n",
    "\n",
    "# First train with ADAM for better convergence -> move the parameters into a\n",
    "# favourable starting positing for BFGS\n",
    "adtype = Optimization.AutoZygote()\n",
    "optf = Optimization.OptimizationFunction((x,p)->loss(x), adtype)\n",
    "optprob = Optimization.OptimizationProblem(optf, ComponentVector{Float64}(p))\n",
    "res1 = Optimization.solve(optprob, ADAM(0.1), callback=callback, maxiters = 200)\n",
    "println(\"Training loss after $(length(losses)) iterations: $(losses[end])\")\n",
    "# Train with BFGS\n",
    "optprob2 = Optimization.OptimizationProblem(optf, res1.minimizer)\n",
    "res2 = Optimization.solve(optprob2, Optim.BFGS(initial_stepnorm=0.01), callback=callback, maxiters = 10000)\n",
    "println(\"Final training loss after $(length(losses)) iterations: $(losses[end])\")\n",
    "\n",
    "# Plot the losses\n",
    "pl_losses = plot(1:200, losses[1:200], yaxis = :log10, xaxis = :log10, xlabel = \"Iterations\", ylabel = \"Loss\", label = \"ADAM\", color = :blue)\n",
    "plot!(201:length(losses), losses[201:end], yaxis = :log10, xaxis = :log10, xlabel = \"Iterations\", ylabel = \"Loss\", label = \"BFGS\", color = :red)\n",
    "savefig(pl_losses, joinpath(pwd(), \"plots\", \"$(svname)_losses.pdf\"))\n",
    "# Rename the best candidate\n",
    "p_trained = res2.minimizer\n",
    "\n",
    "## Analysis of the trained network\n",
    "# Plot the data and the approximation\n",
    "ts = first(solution.t):mean(diff(solution.t))/2:last(solution.t)\n",
    "X̂ = predict(p_trained, Xₙ[:,1], ts)\n",
    "# Trained on noisy data vs real solution\n",
    "pl_trajectory = plot(ts, transpose(X̂), xlabel = \"t\", ylabel =\"x(t), y(t)\", color = :red, label = [\"UDE Approximation\" nothing])\n",
    "scatter!(solution.t, transpose(Xₙ), color = :black, label = [\"Measurements\" nothing])\n",
    "savefig(pl_trajectory, joinpath(pwd(), \"plots\", \"$(svname)_trajectory_reconstruction.pdf\"))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian inference framework with deep learning\n",
    "\n",
    "inspired from https://turinglang.org/dev/tutorials/03-bayesian-neural-network/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [],
   "source": [
    "using Turing\n",
    "using FillArrays\n",
    "using Flux\n",
    "using Plots\n",
    "using ReverseDiff\n",
    "\n",
    "using LinearAlgebra\n",
    "using Random\n",
    "\n",
    "# Use reverse_diff due to the number of parameters in neural networks.\n",
    "Turing.setadbackend(:reversediff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [],
   "source": [
    "# Number of points to generate.\n",
    "N = 80\n",
    "M = round(Int, N / 4)\n",
    "Random.seed!(1234)\n",
    "\n",
    "# Generate artificial data.\n",
    "x1s = rand(M) * 4.5;\n",
    "x2s = rand(M) * 4.5;\n",
    "xt1s = Array([[x1s[i] + 0.5; x2s[i] + 0.5] for i in 1:M])\n",
    "x1s = rand(M) * 4.5;\n",
    "x2s = rand(M) * 4.5;\n",
    "append!(xt1s, Array([[x1s[i] - 5; x2s[i] - 5] for i in 1:M]))\n",
    "\n",
    "x1s = rand(M) * 4.5;\n",
    "x2s = rand(M) * 4.5;\n",
    "xt0s = Array([[x1s[i] + 0.5; x2s[i] - 5] for i in 1:M])\n",
    "x1s = rand(M) * 4.5;\n",
    "x2s = rand(M) * 4.5;\n",
    "append!(xt0s, Array([[x1s[i] - 5; x2s[i] + 0.5] for i in 1:M]))\n",
    "\n",
    "# Store all the data for later.\n",
    "xs = [xt1s; xt0s]\n",
    "ts = [ones(2 * M); zeros(2 * M)]\n",
    "\n",
    "# Plot data points.\n",
    "function plot_data()\n",
    "    x1 = map(e -> e[1], xt1s)\n",
    "    y1 = map(e -> e[2], xt1s)\n",
    "    x2 = map(e -> e[1], xt0s)\n",
    "    y2 = map(e -> e[2], xt0s)\n",
    "\n",
    "    Plots.scatter(x1, y1; color=\"red\", clim=(0, 1))\n",
    "    return Plots.scatter!(x2, y2; color=\"blue\", clim=(0, 1))\n",
    "end\n",
    "\n",
    "plot_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [],
   "source": [
    "# Construct a neural network using Flux\n",
    "nn_initial = Chain(Dense(2, 3, tanh), Dense(3, 2, tanh), Dense(2, 1, σ))\n",
    "\n",
    "# Extract weights and a helper function to reconstruct NN from weights\n",
    "parameters_initial, reconstruct = Flux.destructure(nn_initial)\n",
    "\n",
    "length(parameters_initial) # number of paraemters in NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [],
   "source": [
    "@model function bayes_nn(xs, ts, nparameters, reconstruct; alpha=0.09)\n",
    "    # Create the weight and bias vector.\n",
    "    parameters ~ MvNormal(Zeros(nparameters), I / alpha)\n",
    "\n",
    "    # Construct NN from parameters\n",
    "    nn = reconstruct(parameters)\n",
    "    # Forward NN to make predictions\n",
    "    preds = nn(xs)\n",
    "\n",
    "    # Observe each prediction.\n",
    "    for i in 1:length(ts)\n",
    "        ts[i] ~ Bernoulli(preds[i])\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [],
   "source": [
    "# Perform inference.\n",
    "N = 5000\n",
    "ch = sample(bayes_nn(hcat(xs...), ts, length(parameters_initial), reconstruct), NUTS(), N);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [],
   "source": [
    "# A helper to create NN from weights `theta` and run it through data `x`\n",
    "nn_forward(x, theta) = reconstruct(theta)(x)\n",
    "\n",
    "# Plot the data we have.\n",
    "plot_data()\n",
    "\n",
    "# Find the index that provided the highest log posterior in the chain.\n",
    "_, i = findmax(ch[:lp])\n",
    "\n",
    "# Extract the max row value from i.\n",
    "i = i.I[1]\n",
    "\n",
    "# Plot the posterior distribution with a contour plot\n",
    "x1_range = collect(range(-6; stop=6, length=25))\n",
    "x2_range = collect(range(-6; stop=6, length=25))\n",
    "Z = [nn_forward([x1, x2], theta[i, :])[1] for x1 in x1_range, x2 in x2_range]\n",
    "contour!(x1_range, x2_range, Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [],
   "source": [
    "# Return the average predicted value across\n",
    "# multiple weights.\n",
    "function nn_predict(x, theta, num)\n",
    "    return mean([nn_forward(x, theta[i, :])[1] for i in 1:10:num])\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the average prediction.\n",
    "plot_data()\n",
    "\n",
    "n_end = 1500\n",
    "x1_range = collect(range(-6; stop=6, length=25))\n",
    "x2_range = collect(range(-6; stop=6, length=25))\n",
    "Z = [nn_predict([x1, x2], theta, n_end)[1] for x1 in x1_range, x2 in x2_range]\n",
    "contour!(x1_range, x2_range, Z)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A few packages that I have developed\n",
    "\n",
    "### [EvoId.jl](https://github.com/vboussange/EvoId.jl)\n",
    "Evolutionary Individual based modelling, mathematically grounded. A user friendly package aimed at simulating the evolutionary dynamics of a population structured over a complex spatio-evolutionary structures.\n",
    "\n",
    "### [HighDimPDE.jl](https://github.com/SciML/HighDimPDE.jl)\n",
    "Solver for **highly dimensional, non-local, nonlinear PDEs**. It is integrated within the SciML ecosystem (see below). Try it out! &#128515; If you want to learn more about the algorithms implemented, check out my [research interests]({{site.url}}/research/#developping-numerical-schemes-for-solving-high-dimensional-non-local-nonlinear-pdes).\n",
    "\n",
    "### [PiecewiseInference.jl](https://github.com/vboussange/PiecewiseInference.jl)\n",
    "Suite for parameter inference and model selection with dynamical models characterised by complex dynamics.\n",
    "\n",
    "### [ParametricModels.jl](https://github.com/vboussange/ParametricModels.jl)\n",
    "Utilities for parametric and composite differential equation models.\n",
    "\n",
    "### [EcoEvoModelZoo.jl](https://github.com/vboussange/EcoEvoModelZoo.jl)\n",
    "A zoo of eco-evolutionary models with high fitness.\n",
    "\n",
    "### [SciML](https://github.com/SciML/)\n",
    "I am a member of the **SciML** organisation, an open source ecosystem for Scientific Machine Learning in the Julia programming language. On top of being the main author of **HighDimPDE.jl**, I actively participate in the development of other packages such as [DiffEqFlux.jl](https://github.com/SciML/DiffEqFlux.jl), a library to train differential equations with data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
